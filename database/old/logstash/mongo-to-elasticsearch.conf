input {
	jdbc {
    jdbc_driver_library => "/home/willpcowan/code/projects/arbor-gpt3/back/logstash/MongoDB_JDBCs/dbschema-mongodb-jdbc/MongoDbJdbcDriver/mongojdbc4.0.jar" # Path to MongoDB JDBC driver
    jdbc_driver_class => "Java::com.dbschema.MongoJdbcDriver"
    jdbc_connection_string => "jdbc:mongodb://localhost:27017/arbor"
    jdbc_user => ""
    # jdbc_password => ""
    schedule => "*/10 * * * * *" # Hopefully, every 10 seconds
    statement => "db.concepts.aggregate($match: { 'updatedAt' : {'$gt': ISODate(:sql_last_value), '$lt': new Date() }});" # :sql_last_value is built in
	  tracking_column => "updatedAt" # Track documents read based on the createdAt column (does this miss docs?)
		record_last_run => true # Save the data of last recorded doc into last_run_metadata_path 
	  last_run_metadata_path => "$HOME/code/projects/arbor-gpt3/back/logstash/.logstash_jdbc_last_run"
	}
}
filter {
	# Copy ID before pruning, to give to elasticsearch document (rather than 2 IDs)
  mutate {
    copy => { "_id" => "[@metadata][id]"} 
  }
	# Remove all fields except concept name
	prune { 
		whitelist_names => [ "^name$", "^_id$" ] # ^ and $ allow only 'name', not other patterns
	}
}
output {
	# Output the object to cli, in the 'rudydebug' style
  stdout {
    codec => rubydebug # outputs the same thing as elasticsearch in stdout to facilitate debugging
  }
	# Create document in elasticsearch
  elasticsearch {
    action => "index"           # In an index
    index => "concept_titles"   # Called 'concept_titles'
    hosts => ["localhost:9200"] # At this host
		# document_id => "%{_id}" # Set doc id in ES to same as in MongoDB
		document_id => "%{[document][_id]}" # APPARENTLY THIS WORKS
  }
}